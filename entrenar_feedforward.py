#!/usr/bin/env python3
"""
Entrenamiento FeedForward para clasificación SMAW con VGGish embeddings agregados.

Uso:
    python entrenar_feedforward.py --duration 10 --overlap 0.5 --k-folds 10
"""

import argparse
import hashlib
import json
import pickle
import platform
import subprocess
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import tensorflow_hub as hub
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from torch.optim.swa_utils import SWALR, AveragedModel
from torch.utils.data import DataLoader, Dataset

sys.path.insert(0, str(Path(__file__).parent))
from modelo_feedforward import FeedForwardMultiTask
from utils.audio_utils import PROJECT_ROOT, load_audio_segment
from utils.timing import timer

warnings.filterwarnings("ignore")

# Hiperparámetros
BATCH_SIZE = 32
NUM_EPOCHS = 100
EARLY_STOP_PATIENCE = 15
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 1e-4
LABEL_SMOOTHING = 0.1
SWA_START = 5
VGGISH_MODEL_URL = "https://tfhub.dev/google/vggish/1"

# Cargar modelo VGGish
print(f"Cargando modelo VGGish desde TensorFlow Hub...")
vggish_model = hub.load(VGGISH_MODEL_URL)
print("Modelo VGGish cargado correctamente.")


# ============= Parseo de argumentos =============
def parse_args():
    parser = argparse.ArgumentParser(
        description="Entrenamiento FeedForward SMAW con K-Fold CV"
    )
    parser.add_argument(
        "--duration",
        type=int,
        required=True,
        choices=[1, 2, 5, 10, 20, 30, 50],
        help="Duración de segmento en segundos",
    )
    parser.add_argument(
        "--overlap",
        type=float,
        default=0.5,
        help="Overlap entre segmentos como ratio (0.0 a 0.75, default: 0.5)",
    )
    parser.add_argument(
        "--k-folds",
        type=int,
        default=10,
        choices=[3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
        help="Número de folds para cross-validation (default: 10)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Semilla para reproducibilidad (default: 42)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Dispositivo (cuda/cpu, auto-detectado si no se especifica)",
    )
    return parser.parse_args()


def extract_session_from_path(audio_path: str) -> str:
    """Extrae el identificador de sesión del path del audio."""
    parts = Path(audio_path).parts
    for part in parts:
        if part.endswith("_Audio"):
            return part
    return Path(audio_path).parent.name


def extract_vggish_embeddings_aggregated(
    audio_path: str,
    segment_idx: int,
    segment_duration: float,
    overlap_seconds: float,
) -> np.ndarray:
    """Extrae embeddings VGGish agregados (mean+std) de un segmento."""
    full_path = PROJECT_ROOT / audio_path

    segment = load_audio_segment(
        full_path,
        segment_duration=segment_duration,
        segment_index=segment_idx,
        sr=16000,
        overlap_seconds=overlap_seconds,
    )

    if segment is None:
        raise ValueError(f"No se pudo cargar segmento {segment_idx} de {audio_path}")

    window_size = 16000
    hop_size = 8000
    embeddings_list = []

    for start in range(0, len(segment), hop_size):
        end = start + window_size
        if end > len(segment):
            window = np.zeros(window_size, dtype=np.float32)
            window[: len(segment) - start] = segment[start:]
        else:
            window = segment[start:end]

        embedding = vggish_model(window).numpy()
        embeddings_list.append(embedding[0])

        if end >= len(segment):
            break

    embeddings = np.stack(embeddings_list, axis=0)
    mean = embeddings.mean(axis=0)
    std = embeddings.std(axis=0)
    aggregated = np.concatenate([mean, std], axis=0)

    return aggregated


class AudioDataset(Dataset):
    """Dataset para features agregados."""

    def __init__(self, features, labels_plate, labels_electrode, labels_current):
        self.features = features
        self.labels_plate = labels_plate
        self.labels_electrode = labels_electrode
        self.labels_current = labels_current

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.features[idx], dtype=torch.float32),
            torch.tensor(self.labels_plate[idx], dtype=torch.long),
            torch.tensor(self.labels_electrode[idx], dtype=torch.long),
            torch.tensor(self.labels_current[idx], dtype=torch.long),
        )


def train_one_fold(
    fold_idx,
    train_features,
    train_labels,
    val_features,
    val_labels,
    class_weights,
    encoders,
    device,
    models_dir,
):
    """Entrena un fold y guarda el mejor modelo."""

    plate_encoder, electrode_encoder, current_type_encoder = encoders

    # Crear datasets
    train_dataset = AudioDataset(
        train_features,
        train_labels["plate"],
        train_labels["electrode"],
        train_labels["current"],
    )
    val_dataset = AudioDataset(
        val_features,
        val_labels["plate"],
        val_labels["electrode"],
        val_labels["current"],
    )

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True
    )
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # Crear modelo
    model = FeedForwardMultiTask(input_size=256, hidden_sizes=[512, 256, 128]).to(device)

    # Criterios con class weights
    criterion_plate = nn.CrossEntropyLoss(
        weight=torch.FloatTensor(class_weights["plate"]).to(device),
        label_smoothing=LABEL_SMOOTHING,
    )
    criterion_electrode = nn.CrossEntropyLoss(
        weight=torch.FloatTensor(class_weights["electrode"]).to(device),
        label_smoothing=LABEL_SMOOTHING,
    )
    criterion_current = nn.CrossEntropyLoss(
        weight=torch.FloatTensor(class_weights["current"]).to(device),
        label_smoothing=LABEL_SMOOTHING,
    )

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    swa_model = AveragedModel(model)
    swa_scheduler = SWALR(optimizer, swa_lr=1e-4)

    # Training loop
    best_val_loss = float("inf")
    patience_counter = 0
    best_epoch = 0
    best_metrics = {
        "accuracy_plate": 0.0,
        "accuracy_electrode": 0.0,
        "accuracy_current": 0.0,
        "f1_plate": 0.0,
        "f1_electrode": 0.0,
        "f1_current": 0.0,
    }
    best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}
    training_history = []

    for epoch in range(NUM_EPOCHS):
        # Training
        model.train()
        train_loss = 0.0

        for features, labels_p, labels_e, labels_c in train_loader:
            features = features.to(device)
            labels_p = labels_p.to(device)
            labels_e = labels_e.to(device)
            labels_c = labels_c.to(device)

            optimizer.zero_grad()
            outputs = model(features)

            loss_p = criterion_plate(outputs["logits_espesor"], labels_p)
            loss_e = criterion_electrode(outputs["logits_electrodo"], labels_e)
            loss_c = criterion_current(outputs["logits_corriente"], labels_c)
            loss = (loss_p + loss_e + loss_c) / 3

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()

        # SWA update
        if epoch >= SWA_START:
            swa_model.update_parameters(model)
            swa_scheduler.step()
        else:
            scheduler.step()

        # Validation
        model.eval()
        val_loss = 0.0
        all_preds = {"plate": [], "electrode": [], "current": []}
        all_labels = {"plate": [], "electrode": [], "current": []}

        with torch.no_grad():
            for features, labels_p, labels_e, labels_c in val_loader:
                features = features.to(device)
                labels_p = labels_p.to(device)
                labels_e = labels_e.to(device)
                labels_c = labels_c.to(device)

                outputs = model(features)

                loss_p = criterion_plate(outputs["logits_espesor"], labels_p)
                loss_e = criterion_electrode(outputs["logits_electrodo"], labels_e)
                loss_c = criterion_current(outputs["logits_corriente"], labels_c)
                val_loss += (loss_p + loss_e + loss_c).item()

                _, pred_p = outputs["logits_espesor"].max(1)
                _, pred_e = outputs["logits_electrodo"].max(1)
                _, pred_c = outputs["logits_corriente"].max(1)

                all_preds["plate"].extend(pred_p.cpu().numpy())
                all_preds["electrode"].extend(pred_e.cpu().numpy())
                all_preds["current"].extend(pred_c.cpu().numpy())
                all_labels["plate"].extend(labels_p.cpu().numpy())
                all_labels["electrode"].extend(labels_e.cpu().numpy())
                all_labels["current"].extend(labels_c.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)

        # Calcular métricas
        acc_p = np.mean(np.array(all_preds["plate"]) == np.array(all_labels["plate"]))
        acc_e = np.mean(
            np.array(all_preds["electrode"]) == np.array(all_labels["electrode"])
        )
        acc_c = np.mean(
            np.array(all_preds["current"]) == np.array(all_labels["current"])
        )

        f1_p = f1_score(all_labels["plate"], all_preds["plate"], average="weighted")
        f1_e = f1_score(
            all_labels["electrode"], all_preds["electrode"], average="weighted"
        )
        f1_c = f1_score(all_labels["current"], all_preds["current"], average="weighted")

        training_history.append({
            "epoch": epoch + 1,
            "train_loss": train_loss / len(train_loader),
            "val_loss": avg_val_loss,
            "val_acc_plate": acc_p,
            "val_acc_electrode": acc_e,
            "val_acc_current": acc_c,
            "val_f1_plate": f1_p,
            "val_f1_electrode": f1_e,
            "val_f1_current": f1_c,
        })

        # Early stopping y guardar mejor modelo
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_epoch = epoch + 1
            best_metrics = {
                "accuracy_plate": acc_p,
                "accuracy_electrode": acc_e,
                "accuracy_current": acc_c,
                "f1_plate": f1_p,
                "f1_electrode": f1_e,
                "f1_current": f1_c,
            }
            best_state_dict = {
                k: v.cpu().clone() for k, v in model.state_dict().items()
            }
        else:
            patience_counter += 1
            if patience_counter >= EARLY_STOP_PATIENCE:
                break

    # Guardar el mejor modelo de este fold
    model_path = models_dir / f"model_fold_{fold_idx}.pth"
    torch.save(best_state_dict, model_path)

    # Calcular matrices de confusión del mejor modelo
    model.load_state_dict(best_state_dict)
    model.eval()

    val_preds = {"plate": [], "electrode": [], "current": []}
    val_labels_all = {"plate": [], "electrode": [], "current": []}

    with torch.no_grad():
        for features, labels_p, labels_e, labels_c in val_loader:
            features = features.to(device)
            outputs = model(features)

            _, pred_p = outputs["logits_espesor"].max(1)
            _, pred_e = outputs["logits_electrodo"].max(1)
            _, pred_c = outputs["logits_corriente"].max(1)

            val_preds["plate"].extend(pred_p.cpu().numpy())
            val_preds["electrode"].extend(pred_e.cpu().numpy())
            val_preds["current"].extend(pred_c.cpu().numpy())
            val_labels_all["plate"].extend(labels_p.numpy())
            val_labels_all["electrode"].extend(labels_e.numpy())
            val_labels_all["current"].extend(labels_c.numpy())

    cm_plate = confusion_matrix(val_labels_all["plate"], val_preds["plate"])
    cm_electrode = confusion_matrix(val_labels_all["electrode"], val_preds["electrode"])
    cm_current = confusion_matrix(val_labels_all["current"], val_preds["current"])

    best_metrics["confusion_matrix_plate"] = cm_plate.tolist()
    best_metrics["confusion_matrix_electrode"] = cm_electrode.tolist()
    best_metrics["confusion_matrix_current"] = cm_current.tolist()

    print(
        f"  Fold {fold_idx + 1}: Plate={best_metrics['accuracy_plate']:.4f} | "
        f"Electrode={best_metrics['accuracy_electrode']:.4f} | "
        f"Current={best_metrics['accuracy_current']:.4f} | "
        f"Best epoch={best_epoch}"
    )

    return best_metrics, best_epoch, training_history


def ensemble_predict(models, features, device):
    """Realiza predicciones usando voting de múltiples modelos."""
    all_logits_plate = []
    all_logits_electrode = []
    all_logits_current = []

    features_tensor = torch.tensor(features, dtype=torch.float32).to(device)

    for model in models:
        model.eval()
        with torch.no_grad():
            outputs = model(features_tensor)
            all_logits_plate.append(outputs["logits_espesor"])
            all_logits_electrode.append(outputs["logits_electrodo"])
            all_logits_current.append(outputs["logits_corriente"])

    avg_logits_plate = torch.stack(all_logits_plate).mean(dim=0)
    avg_logits_electrode = torch.stack(all_logits_electrode).mean(dim=0)
    avg_logits_current = torch.stack(all_logits_current).mean(dim=0)

    pred_plate = avg_logits_plate.argmax(dim=1)
    pred_electrode = avg_logits_electrode.argmax(dim=1)
    pred_current = avg_logits_current.argmax(dim=1)

    return pred_plate, pred_electrode, pred_current


def get_system_info(device):
    """Recolecta información del sistema y dependencias."""
    info = {
        "python_version": platform.python_version(),
        "torch_version": torch.__version__,
        "numpy_version": np.__version__,
        "platform": platform.platform(),
        "device": str(device),
    }
    try:
        import tensorflow as tf

        info["tensorflow_version"] = tf.__version__
    except Exception:
        pass
    if device.type == "cuda":
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_gb"] = round(
            torch.cuda.get_device_properties(0).total_memory / 1e9, 2
        )
    return info


def count_model_parameters(model):
    """Cuenta parámetros totales y entrenables del modelo."""
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return {"total": total, "trainable": trainable}


# ============= Main =============


def main():
    start_time = time.time()

    args = parse_args()
    SEGMENT_DURATION = float(args.duration)
    OVERLAP_RATIO = args.overlap
    OVERLAP_SECONDS = SEGMENT_DURATION * OVERLAP_RATIO
    N_FOLDS = args.k_folds
    RANDOM_SEED = args.seed

    device = args.device or ("cuda" if torch.cuda.is_available() else "cpu")
    device = torch.device(device)

    # Directorios
    ROOT_DIR = Path(__file__).parent
    DURATION_DIR = ROOT_DIR / f"{int(SEGMENT_DURATION):02d}seg"
    DURATION_DIR.mkdir(exist_ok=True)

    MODELS_BASE_DIR = DURATION_DIR / "modelos" / "feedforward"
    MODELS_BASE_DIR.mkdir(exist_ok=True, parents=True)
    MODELS_DIR = MODELS_BASE_DIR / f"k{N_FOLDS:02d}_overlap_{OVERLAP_RATIO}"
    MODELS_DIR.mkdir(exist_ok=True)

    print(f"\n{'=' * 70}")
    print(f"ENTRENAMIENTO FEEDFORWARD")
    print(f"{'=' * 70}")
    print(f"Dispositivo: {device}")
    print(f"Duración de segmento: {SEGMENT_DURATION}s")
    print(f"Overlap: {OVERLAP_RATIO} ({OVERLAP_SECONDS}s)")
    print(f"K-Folds: {N_FOLDS}")
    print(f"Semilla: {RANDOM_SEED}")
    print(f"Directorio datos: {DURATION_DIR}/")
    print(f"Modelos se guardarán en: {MODELS_DIR}/")

    # Cargar CSVs
    with timer("Cargar CSVs (train/test)"):
        train_csv = DURATION_DIR / f"train_overlap_{OVERLAP_RATIO}.csv"
        test_csv = DURATION_DIR / f"test_overlap_{OVERLAP_RATIO}.csv"
        if not train_csv.exists():
            train_csv = DURATION_DIR / "train.csv"
        if not test_csv.exists():
            test_csv = DURATION_DIR / "test.csv"
        train_data = pd.read_csv(train_csv)
        test_data = pd.read_csv(test_csv)
        all_data = pd.concat([train_data, test_data], ignore_index=True)

    print(f"\nTotal de segmentos: {len(all_data)}")

    # Extraer sesión
    all_data["Session"] = all_data["Audio Path"].apply(extract_session_from_path)
    print(f"Sesiones únicas: {all_data['Session'].nunique()}")

    # Encoders
    plate_encoder = LabelEncoder()
    electrode_encoder = LabelEncoder()
    current_type_encoder = LabelEncoder()

    plate_encoder.fit(all_data["Plate Thickness"])
    electrode_encoder.fit(all_data["Electrode"])
    current_type_encoder.fit(all_data["Type of Current"])

    all_data["Plate Encoded"] = plate_encoder.transform(all_data["Plate Thickness"])
    all_data["Electrode Encoded"] = electrode_encoder.transform(all_data["Electrode"])
    all_data["Current Encoded"] = current_type_encoder.transform(
        all_data["Type of Current"]
    )

    # Extraer embeddings VGGish agregados
    print("\nExtrayendo embeddings VGGish agregados de todos los segmentos...")
    paths = all_data["Audio Path"].values
    segment_indices = all_data["Segment Index"].values

    all_features = []
    for i, (path, seg_idx) in enumerate(zip(paths, segment_indices)):
        if i % 100 == 0:
            print(f"  Procesando {i}/{len(paths)}...")
        feat = extract_vggish_embeddings_aggregated(
            path, int(seg_idx), SEGMENT_DURATION, OVERLAP_SECONDS
        )
        all_features.append(feat)

    all_features = np.array(all_features)
    print(f"Features extraídos: {all_features.shape}")

    # Preparar arrays
    y_plate = all_data["Plate Encoded"].values
    y_electrode = all_data["Electrode Encoded"].values
    y_current = all_data["Current Encoded"].values
    sessions = all_data["Session"].values
    y_stratify = y_electrode

    # ============= FASE 1: Entrenar K modelos =============
    print(f"\n{'=' * 70}")
    print(f"FASE 1: ENTRENAMIENTO DE {N_FOLDS} MODELOS (StratifiedGroupKFold)")
    print(f"{'=' * 70}")

    training_start_time = time.time()

    sgkf = StratifiedGroupKFold(
        n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED
    )
    fold_metrics = []
    fold_best_epochs = []
    fold_training_times = []
    all_fold_histories = []

    for fold_idx, (train_idx, val_idx) in enumerate(
        sgkf.split(all_features, y_stratify, groups=sessions)
    ):
        train_sessions = set(sessions[train_idx])
        val_sessions = set(sessions[val_idx])
        assert len(train_sessions & val_sessions) == 0, "ERROR: Sesiones mezcladas!"

        print(f"\nFold {fold_idx + 1}/{N_FOLDS}")
        print(f"  Train: {len(train_idx)} segmentos ({len(train_sessions)} sesiones)")
        print(f"  Val: {len(val_idx)} segmentos ({len(val_sessions)} sesiones)")

        X_train = all_features[train_idx]
        X_val = all_features[val_idx]

        train_labels = {
            "plate": y_plate[train_idx],
            "electrode": y_electrode[train_idx],
            "current": y_current[train_idx],
        }
        val_labels = {
            "plate": y_plate[val_idx],
            "electrode": y_electrode[val_idx],
            "current": y_current[val_idx],
        }

        class_weights = {
            "plate": compute_class_weight(
                "balanced",
                classes=np.unique(train_labels["plate"]),
                y=train_labels["plate"],
            ),
            "electrode": compute_class_weight(
                "balanced",
                classes=np.unique(train_labels["electrode"]),
                y=train_labels["electrode"],
            ),
            "current": compute_class_weight(
                "balanced",
                classes=np.unique(train_labels["current"]),
                y=train_labels["current"],
            ),
        }

        fold_start_time = time.time()
        with timer(f"Entrenar fold {fold_idx + 1}/{N_FOLDS}"):
            metrics, best_epoch, fold_history = train_one_fold(
                fold_idx,
                X_train,
                train_labels,
                X_val,
                val_labels,
                class_weights,
                (plate_encoder, electrode_encoder, current_type_encoder),
                device,
                MODELS_DIR,
            )
        fold_time = time.time() - fold_start_time

        metrics["time_seconds"] = round(fold_time, 2)
        metrics["fold"] = fold_idx
        fold_metrics.append(metrics)
        fold_best_epochs.append(best_epoch)
        fold_training_times.append(round(fold_time, 2))
        all_fold_histories.append(fold_history)

    training_end_time = time.time()
    training_time = training_end_time - training_start_time
    training_time_minutes = training_time / 60
    print(
        f"\nTiempo de entrenamiento puro: {training_time:.2f}s ({training_time_minutes:.2f}min)"
    )

    # ============= FASE 2: Evaluar Ensemble =============
    print(f"\n{'=' * 70}")
    print("FASE 2: EVALUACIÓN DEL ENSEMBLE (Soft Voting)")
    print(f"{'=' * 70}")

    sample_model = FeedForwardMultiTask(input_size=256, hidden_sizes=[512, 256, 128])
    model_params = count_model_parameters(sample_model)
    del sample_model

    with timer("Cargar modelos del ensemble"):
        models = []
        for fold_idx in range(N_FOLDS):
            model = FeedForwardMultiTask(
                input_size=256, hidden_sizes=[512, 256, 128]
            ).to(device)
            model.load_state_dict(torch.load(MODELS_DIR / f"model_fold_{fold_idx}.pth"))
            model.eval()
            models.append(model)
        print(f"Cargados {len(models)} modelos del ensemble")

    # Evaluar en todo el dataset
    all_preds = {"plate": [], "electrode": [], "current": []}
    all_labels = {"plate": [], "electrode": [], "current": []}

    pred_p, pred_e, pred_c = ensemble_predict(models, all_features, device)
    all_preds["plate"].extend(pred_p.cpu().numpy())
    all_preds["electrode"].extend(pred_e.cpu().numpy())
    all_preds["current"].extend(pred_c.cpu().numpy())
    all_labels["plate"].extend(y_plate)
    all_labels["electrode"].extend(y_electrode)
    all_labels["current"].extend(y_current)

    # Calcular métricas del ensemble
    acc_p = np.mean(np.array(all_preds["plate"]) == np.array(all_labels["plate"]))
    acc_e = np.mean(
        np.array(all_preds["electrode"]) == np.array(all_labels["electrode"])
    )
    acc_c = np.mean(np.array(all_preds["current"]) == np.array(all_labels["current"]))

    f1_p = f1_score(all_labels["plate"], all_preds["plate"], average="weighted")
    f1_e = f1_score(all_labels["electrode"], all_preds["electrode"], average="weighted")
    f1_c = f1_score(all_labels["current"], all_preds["current"], average="weighted")

    prec_p = precision_score(
        all_labels["plate"], all_preds["plate"], average="weighted"
    )
    prec_e = precision_score(
        all_labels["electrode"], all_preds["electrode"], average="weighted"
    )
    prec_c = precision_score(
        all_labels["current"], all_preds["current"], average="weighted"
    )

    rec_p = recall_score(all_labels["plate"], all_preds["plate"], average="weighted")
    rec_e = recall_score(
        all_labels["electrode"], all_preds["electrode"], average="weighted"
    )
    rec_c = recall_score(
        all_labels["current"], all_preds["current"], average="weighted"
    )

    avg_acc_p = np.mean([m["accuracy_plate"] for m in fold_metrics])
    avg_acc_e = np.mean([m["accuracy_electrode"] for m in fold_metrics])
    avg_acc_c = np.mean([m["accuracy_current"] for m in fold_metrics])

    print(f"\n{'=' * 70}")
    print("RESULTADOS FINALES")
    print(f"{'=' * 70}")

    print("\nMétricas individuales por fold (promedio):")
    print(f"  Plate:     {avg_acc_p:.4f}")
    print(f"  Electrode: {avg_acc_e:.4f}")
    print(f"  Current:   {avg_acc_c:.4f}")

    print(f"\nMétricas del ENSEMBLE (Soft Voting, {N_FOLDS} modelos):")
    print(
        f"  Plate:     Acc={acc_p:.4f} | F1={f1_p:.4f} | Prec={prec_p:.4f} | Rec={rec_p:.4f}"
    )
    print(
        f"  Electrode: Acc={acc_e:.4f} | F1={f1_e:.4f} | Prec={prec_e:.4f} | Rec={rec_e:.4f}"
    )
    print(
        f"  Current:   Acc={acc_c:.4f} | F1={f1_c:.4f} | Prec={prec_c:.4f} | Rec={rec_c:.4f}"
    )

    print(f"\nMejora del Ensemble vs Promedio Individual:")
    print(f"  Plate:     {acc_p - avg_acc_p:+.4f}")
    print(f"  Electrode: {acc_e - avg_acc_e:+.4f}")
    print(f"  Current:   {acc_c - avg_acc_c:+.4f}")

    print(f"\n{'=' * 70}")
    print("REPORTES DE CLASIFICACIÓN")
    print(f"{'=' * 70}")

    print("\n--- Plate Thickness ---")
    print(
        classification_report(
            all_labels["plate"],
            all_preds["plate"],
            target_names=plate_encoder.classes_,
            zero_division=0,
        )
    )

    print("\n--- Electrode Type ---")
    print(
        classification_report(
            all_labels["electrode"],
            all_preds["electrode"],
            target_names=electrode_encoder.classes_,
            zero_division=0,
        )
    )

    print("\n--- Type of Current ---")
    print(
        classification_report(
            all_labels["current"],
            all_preds["current"],
            target_names=current_type_encoder.classes_,
            zero_division=0,
        )
    )

    print(f"\n{'=' * 70}")
    print("MATRICES DE CONFUSIÓN")
    print(f"{'=' * 70}")

    cm_plate = confusion_matrix(all_labels["plate"], all_preds["plate"])
    cm_electrode = confusion_matrix(all_labels["electrode"], all_preds["electrode"])
    cm_current = confusion_matrix(all_labels["current"], all_preds["current"])

    print("\nPlate Thickness:")
    print(cm_plate)
    print(f"Clases: {plate_encoder.classes_}")

    print("\nElectrode Type:")
    print(cm_electrode)
    print(f"Clases: {electrode_encoder.classes_}")

    print("\nType of Current:")
    print(cm_current)
    print(f"Clases: {current_type_encoder.classes_}")

    # Guardar resultados
    end_time = time.time()
    elapsed_time = end_time - start_time
    elapsed_minutes = elapsed_time / 60
    elapsed_hours = elapsed_time / 3600

    segments_per_class = {
        "plate": all_data["Plate Thickness"].value_counts().to_dict(),
        "electrode": all_data["Electrode"].value_counts().to_dict(),
        "current": all_data["Type of Current"].value_counts().to_dict(),
    }

    new_entry = {
        "id": f"{int(SEGMENT_DURATION)}seg_{N_FOLDS}fold_overlap_{OVERLAP_RATIO}_feedforward_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "timestamp": datetime.now().isoformat(),
        "model_type": "feedforward",
        "execution_time": {
            "seconds": round(elapsed_time, 2),
            "minutes": round(elapsed_minutes, 2),
            "hours": round(elapsed_hours, 4),
        },
        "training_time": {
            "seconds": round(training_time, 2),
            "minutes": round(training_time_minutes, 2),
        },
        "config": {
            "segment_duration": SEGMENT_DURATION,
            "overlap_ratio": OVERLAP_RATIO,
            "overlap_seconds": OVERLAP_SECONDS,
            "n_folds": N_FOLDS,
            "models_dir": str(MODELS_DIR.name),
            "random_seed": RANDOM_SEED,
            "voting_method": "soft",
            "batch_size": BATCH_SIZE,
            "epochs": NUM_EPOCHS,
            "learning_rate": LEARNING_RATE,
            "weight_decay": WEIGHT_DECAY,
            "label_smoothing": LABEL_SMOOTHING,
            "early_stop_patience": EARLY_STOP_PATIENCE,
            "swa_start": SWA_START,
            "swa_lr": 1e-4,
        },
        "system_info": get_system_info(device),
        "model_parameters": model_params,
        "data": {
            "total_segments": len(all_data),
            "unique_sessions": int(all_data["Session"].nunique()),
            "segments_per_class": segments_per_class,
            "classes": {
                "plate": list(plate_encoder.classes_),
                "electrode": list(electrode_encoder.classes_),
                "current": list(current_type_encoder.classes_),
            },
        },
        "fold_results": fold_metrics,
        "fold_best_epochs": fold_best_epochs,
        "fold_training_times_seconds": fold_training_times,
        "ensemble_results": {
            "plate": {
                "accuracy": round(acc_p, 4),
                "f1": round(f1_p, 4),
                "precision": round(prec_p, 4),
                "recall": round(rec_p, 4),
            },
            "electrode": {
                "accuracy": round(acc_e, 4),
                "f1": round(f1_e, 4),
                "precision": round(prec_e, 4),
                "recall": round(rec_e, 4),
            },
            "current": {
                "accuracy": round(acc_c, 4),
                "f1": round(f1_c, 4),
                "precision": round(prec_c, 4),
                "recall": round(rec_c, 4),
            },
        },
        "individual_avg": {
            "plate": round(avg_acc_p, 4),
            "electrode": round(avg_acc_e, 4),
            "current": round(avg_acc_c, 4),
        },
        "improvement_vs_individual": {
            "plate": round(acc_p - avg_acc_p, 4),
            "electrode": round(acc_e - avg_acc_e, 4),
            "current": round(acc_c - avg_acc_c, 4),
        },
        "training_history": all_fold_histories,
    }

    # Cargar historial existente o crear nuevo
    results_path = DURATION_DIR / "resultados.json"
    if results_path.exists():
        with open(results_path, "r") as f:
            history = json.load(f)
        if not isinstance(history, list):
            history = [history]
    else:
        history = []

    history.append(new_entry)

    with open(results_path, "w") as f:
        json.dump(history, f, indent=2)

    print(f"\nResultados guardados en: {results_path} (entrada #{len(history)})")
    print(f"Modelos guardados en: {MODELS_DIR}/")
    print(
        f"\nTiempo de ejecución: {elapsed_time:.2f}s ({elapsed_minutes:.2f}min / {elapsed_hours:.4f}h)"
    )


if __name__ == "__main__":
    main()
